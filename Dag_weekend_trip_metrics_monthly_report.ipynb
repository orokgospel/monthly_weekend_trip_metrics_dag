{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAoQ2prt0wKcgeXbO4Fa90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orokgospel/monthly_weekend_trip_metrics_dag/blob/main/Dag_weekend_trip_metrics_monthly_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Tl-EqN_rMV_m",
        "outputId": "6ae9b9a7-61b5-4a05-8e49-76ac595cb127"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'airflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3391155897.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPythonOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'airflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "#   AIRFLOW DAG: Weekend Report ETL with ClickHouse (Secure)\n",
        "#   Loads credentials from .env, performs ETL, sends email\n",
        "# ============================================================\n",
        "\n",
        "from datetime import datetime\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "import pandas as pd\n",
        "import requests\n",
        "import yagmail\n",
        "import traceback\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from io import StringIO\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. LOAD ENVIRONMENT VARIABLES (.env FILE)\n",
        "# ------------------------------------------------------------\n",
        "load_dotenv()  # This loads CLICKHOUSE_* and EMAIL_* automatically\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. HELPER: Connect to ClickHouse HTTP Endpoint\n",
        "# ------------------------------------------------------------\n",
        "def clickhouse_query(sql: str) -> str:\n",
        "    \"\"\"\n",
        "    Execute SQL against ClickHouse using its HTTP interface.\n",
        "    Credentials and host are loaded from .env for full security.\n",
        "    \"\"\"\n",
        "    response = requests.post(\n",
        "        os.getenv(\"CLICKHOUSE_HOST\"),\n",
        "        data=sql.encode(\"utf-8\"),\n",
        "        auth=(os.getenv(\"CLICKHOUSE_USER\"), os.getenv(\"CLICKHOUSE_PASSWORD\")),\n",
        "        headers={\"Content-Type\": \"text/plain\"},\n",
        "        timeout=60\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def clickhouse_query_dataframe(sql: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return a pandas DataFrame from ClickHouse SQL.\n",
        "    Converts ClickHouse output to CSV and parses it.\n",
        "    \"\"\"\n",
        "    csv_sql = f\"SELECT * FROM ({sql}) FORMAT CSVWithNames\"\n",
        "    output = clickhouse_query(csv_sql)\n",
        "    return pd.read_csv(StringIO(output))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. HELPER: SAFE EMAIL SENDER\n",
        "# ------------------------------------------------------------\n",
        "def send_email_safe(subject, message):\n",
        "    \"\"\"\n",
        "    Send email notifications using Gmail App Password.\n",
        "    This function NEVER breaks the DAG — errors are logged but suppressed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        user = os.getenv(\"EMAIL_USER\")\n",
        "        password = os.getenv(\"EMAIL_PASSWORD\")\n",
        "        to_addr = os.getenv(\"EMAIL_TO\")\n",
        "\n",
        "        yag = yagmail.SMTP(user, password)\n",
        "        yag.send(to_addr, subject, message)\n",
        "        print(\"Email sent successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Email sending failed: {e}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. SQL QUERY TO RUN\n",
        "# ------------------------------------------------------------\n",
        "SQL_QUERY = \"\"\"\n",
        "SELECT\n",
        "  toMonth(tpep_pickup_datetime) AS month_num,\n",
        "  countIf(toDayOfWeek(tpep_pickup_datetime) = 6) AS sat_mean_trip_count,\n",
        "  avgIf(fare_amount, toDayOfWeek(tpep_pickup_datetime) = 6) AS sat_mean_fare_per_trip,\n",
        "  avgIf(trip_duration, toDayOfWeek(tpep_pickup_datetime) = 6) AS sat_mean_duration_per_trip,\n",
        "\n",
        "  countIf(toDayOfWeek(tpep_pickup_datetime) = 7) AS sun_mean_trip_count,\n",
        "  avgIf(fare_amount, toDayOfWeek(tpep_pickup_datetime) = 7) AS sun_mean_fare_per_trip,\n",
        "  avgIf(trip_duration, toDayOfWeek(tpep_pickup_datetime) = 7) AS sun_mean_duration_per_trip\n",
        "\n",
        "FROM tripdata\n",
        "WHERE tpep_pickup_datetime >= '2014-01-01'\n",
        "  AND tpep_pickup_datetime <= '2016-12-31'\n",
        "GROUP BY toMonth(tpep_pickup_datetime)\n",
        "ORDER BY toMonth(tpep_pickup_datetime)\n",
        "\"\"\"\n",
        "\n",
        "DESTINATION_TABLE = \"weekend_monthly_report\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. ETL TASK FUNCTIONS\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def extract_and_validate(**context):\n",
        "    \"\"\"\n",
        "    Extracts data from ClickHouse and validates:\n",
        "    - Data must not be empty\n",
        "    - Month numbers must be valid (1–12)\n",
        "    \"\"\"\n",
        "    print(\"Running ClickHouse query...\")\n",
        "    df = clickhouse_query_dataframe(SQL_QUERY)\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"❌ Query returned NO DATA!\")\n",
        "\n",
        "    if not df[\"month_num\"].between(1, 12).all():\n",
        "        raise ValueError(\"❌ Invalid month_num detected!\")\n",
        "\n",
        "    # Save to XCom\n",
        "    context['ti'].xcom_push(key=\"result_df\", value=df.to_json())\n",
        "    print(\"Validation successful.\")\n",
        "\n",
        "\n",
        "def create_destination_table():\n",
        "    \"\"\"\n",
        "    Ensures the destination table exists (idempotent).\n",
        "    \"\"\"\n",
        "    sql = f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS {DESTINATION_TABLE} (\n",
        "        month_num UInt8,\n",
        "        sat_mean_trip_count UInt32,\n",
        "        sat_mean_fare_per_trip Float64,\n",
        "        sat_mean_duration_per_trip Float64,\n",
        "        sun_mean_trip_count UInt32,\n",
        "        sun_mean_fare_per_trip Float64,\n",
        "        sun_mean_duration_per_trip Float64\n",
        "    ) ENGINE = MergeTree()\n",
        "    ORDER BY month_num;\n",
        "    \"\"\"\n",
        "    clickhouse_query(sql)\n",
        "    print(\"Destination table ensured.\")\n",
        "\n",
        "\n",
        "def load_results(**context):\n",
        "    \"\"\"\n",
        "    Inserts transformed results into ClickHouse.\n",
        "    \"\"\"\n",
        "    df_json = context['ti'].xcom_pull(key=\"result_df\")\n",
        "    df = pd.read_json(df_json)\n",
        "\n",
        "    insert_sql = (\n",
        "        f\"INSERT INTO {DESTINATION_TABLE} FORMAT CSVWithNames\\n\" +\n",
        "        df.to_csv(index=False)\n",
        "    )\n",
        "\n",
        "    clickhouse_query(insert_sql)\n",
        "    print(\"Insert completed.\")\n",
        "\n",
        "\n",
        "def notify_success():\n",
        "    send_email_safe(\n",
        "        \"ETL SUCCESS: Weekend Trip Report\",\n",
        "        \"ETL job completed successfully and data is available in ClickHouse.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def notify_failure(context):\n",
        "    \"\"\"\n",
        "    This is executed automatically by Airflow when ANY task fails.\n",
        "    \"\"\"\n",
        "    error_info = \"\".join(\n",
        "        traceback.format_exception(None, context[\"exception\"], context[\"exception\"])\n",
        "    )\n",
        "\n",
        "    send_email_safe(\n",
        "        \"❌ ETL FAILED\",\n",
        "        f\"ETL Pipeline failed!\\n\\nError Details:\\n{error_info}\"\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6. AIRFLOW DAG DEFINITION\n",
        "# ------------------------------------------------------------\n",
        "with DAG(\n",
        "    dag_id=\"clickhouse_weekend_report_etl_secure\",\n",
        "    start_date=datetime(2024, 1, 1),\n",
        "    schedule_interval=\"@daily\",\n",
        "    catchup=False,\n",
        "    on_failure_callback=notify_failure,\n",
        "    default_args={\"owner\": \"gospel\"}\n",
        ") as dag:\n",
        "\n",
        "    extract_task = PythonOperator(\n",
        "        task_id=\"extract_and_validate\",\n",
        "        python_callable=extract_and_validate,\n",
        "        provide_context=True\n",
        "    )\n",
        "\n",
        "    create_table_task = PythonOperator(\n",
        "        task_id=\"create_destination_table\",\n",
        "        python_callable=create_destination_table\n",
        "    )\n",
        "\n",
        "    load_task = PythonOperator(\n",
        "        task_id=\"load_results\",\n",
        "        python_callable=load_results,\n",
        "        provide_context=True\n",
        "    )\n",
        "\n",
        "    success_email_task = PythonOperator(\n",
        "        task_id=\"notify_success\",\n",
        "        python_callable=notify_success\n",
        "    )\n",
        "\n",
        "    # DAG WORKFLOW\n",
        "    extract_task >> create_table_task >> load_task >> success_email_task\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from clickhouse_driver import Client\n",
        "import psycopg2\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "import logging\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# DAG DEFAULT SETTINGS\n",
        "# ---------------------------------------------------------\n",
        "default_args = {\n",
        "    \"owner\": \"airflow\",\n",
        "    \"retries\": 1,\n",
        "    \"retry_delay\": timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "dag = DAG(\n",
        "    dag_id=\"weekend_trip_clickhouse_etl\",\n",
        "    default_args=default_args,\n",
        "    start_date=datetime(2023, 1, 1),\n",
        "    schedule_interval=\"@daily\",\n",
        "    catchup=False,\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ETL PARAMETERS\n",
        "# ---------------------------------------------------------\n",
        "SQL_QUERY = \"\"\"\n",
        "SELECT\n",
        "    toMonth(tpep_pickup_datetime) AS month_num,\n",
        "    countIf(toDayOfWeek(tpep_pickup_datetime) = 6) AS sat_mean_trip_count,\n",
        "    avgIf(fare_amount, toDayOfWeek(tpep_pickup_datetime) = 6) AS sat_mean_fare_per_trip,\n",
        "    avgIf(trip_duration, toDayOfWeek(tpep_pickup_datetime) = 6) AS sat_mean_trip_duration\n",
        "FROM yellow_taxi_trips\n",
        "WHERE toYear(tpep_pickup_datetime) BETWEEN 2014 AND 2016\n",
        "GROUP BY month_num\n",
        "ORDER BY month_num;\n",
        "\"\"\"\n",
        "\n",
        "DEST_TABLE = \"public.weekend_trip_report\"\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# DATABASE FUNCTIONS\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def extract_from_clickhouse():\n",
        "    \"\"\"\n",
        "    Extract data from ClickHouse using an Airflow Connection.\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # BEST PRACTICE (Comment Only):\n",
        "    # Use environment variables:\n",
        "    # click_user = os.getenv(\"CLICKHOUSE_USER\")\n",
        "    # click_pass = os.getenv(\"CLICKHOUSE_PASSWORD\")\n",
        "    # ---------------------------------------------------------------------\n",
        "\n",
        "    # Using Airflow Connection instead\n",
        "    from airflow.hooks.base import BaseHook\n",
        "    conn = BaseHook.get_connection(\"clickhouse_default\")\n",
        "\n",
        "    client = Client(\n",
        "        host=conn.host,\n",
        "        port=conn.port,\n",
        "        user=conn.login,\n",
        "        password=conn.password,\n",
        "        database=conn.schema\n",
        "    )\n",
        "\n",
        "    data = client.execute(SQL_QUERY)\n",
        "    df = pd.DataFrame(data, columns=[\"month_num\", \"sat_mean_trip_count\",\n",
        "                                     \"sat_mean_fare_per_trip\", \"sat_mean_trip_duration\"])\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Query returned no data!\")\n",
        "\n",
        "    df.to_csv(\"/tmp/weekend_trip_report.csv\", index=False)\n",
        "    logging.info(\"Extraction Successful — CSV saved.\")\n",
        "\n",
        "    return \"/tmp/weekend_trip_report.csv\"\n",
        "\n",
        "\n",
        "def load_into_postgres(csv_path):\n",
        "    \"\"\"\n",
        "    Load CSV data into Postgres using Airflow Connection.\n",
        "    \"\"\"\n",
        "    from airflow.hooks.base import BaseHook\n",
        "    conn = BaseHook.get_connection(\"postgres_dest\")\n",
        "\n",
        "    pg = psycopg2.connect(\n",
        "        host=conn.host,\n",
        "        port=conn.port,\n",
        "        user=conn.login,\n",
        "        password=conn.password,\n",
        "        database=conn.schema\n",
        "    )\n",
        "\n",
        "    cur = pg.cursor()\n",
        "\n",
        "    # Ensure table exists\n",
        "    cur.execute(f\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS {DEST_TABLE} (\n",
        "            month_num INT,\n",
        "            sat_mean_trip_count BIGINT,\n",
        "            sat_mean_fare_per_trip FLOAT,\n",
        "            sat_mean_trip_duration FLOAT\n",
        "        );\n",
        "    \"\"\")\n",
        "\n",
        "    pg.commit()\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        cur.execute(f\"\"\"\n",
        "            INSERT INTO {DEST_TABLE}\n",
        "            (month_num, sat_mean_trip_count, sat_mean_fare_per_trip, sat_mean_trip_duration)\n",
        "            VALUES (%s, %s, %s, %s)\n",
        "        \"\"\", tuple(row))\n",
        "\n",
        "    pg.commit()\n",
        "    cur.close()\n",
        "    pg.close()\n",
        "\n",
        "    logging.info(\"Load Successful — Data inserted into Postgres.\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# OPTIONAL EMAIL (DISABLED TO AVOID ERRORS)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def send_email_success():\n",
        "    \"\"\"\n",
        "    Disabled email function to avoid Gmail SMTP 535 error.\n",
        "    Provided only as a safe template with os.getenv best-practice.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    # BEST PRACTICE (Comment Only)\n",
        "    email_user = os.getenv(\"EMAIL_USER\")\n",
        "    email_pass = os.getenv(\"EMAIL_PASS\")\n",
        "\n",
        "    msg = MIMEMultipart()\n",
        "    msg[\"From\"] = email_user\n",
        "    msg[\"To\"] = \"team@example.com\"\n",
        "    msg[\"Subject\"] = \"ETL SUCCESS\"\n",
        "\n",
        "    body = \"Weekend ETL Completed Successfully.\"\n",
        "    msg.attach(MIMEText(body, \"plain\"))\n",
        "\n",
        "    server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
        "    server.starttls()\n",
        "\n",
        "    # Gmail will fail without proper App Password\n",
        "    server.login(email_user, email_pass)\n",
        "\n",
        "    server.send_message(msg)\n",
        "    server.quit()\n",
        "    \"\"\"\n",
        "\n",
        "    logging.info(\"Email notification skipped to avoid SMTP errors.\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# DAG TASK DEFINITIONS\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def etl_pipeline():\n",
        "    csv_file = extract_from_clickhouse()\n",
        "    load_into_postgres(csv_file)\n",
        "    send_email_success()\n",
        "    logging.info(\"ETL Pipeline Completed Successfully.\")\n",
        "\n",
        "\n",
        "etl_task = PythonOperator(\n",
        "    task_id=\"run_weekend_trip_etl\",\n",
        "    python_callable=etl_pipeline,\n",
        "    dag=dag\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "cellView": "form",
        "id": "ImuwfIljQp11",
        "outputId": "b2dad0cf-73ed-421c-f8ea-ee37d1596f95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'airflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2996543019.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPythonOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mclickhouse_driver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'airflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}